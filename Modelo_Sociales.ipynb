{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jpcan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jpcan\\OneDrive\\Documentos\\Andes Universidad\\Analitica computacional\\Proyecto_final_github\\Limpiezadatos.py:23: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, engine)\n",
      "c:\\Users\\jpcan\\OneDrive\\Documentos\\Andes Universidad\\Analitica computacional\\Proyecto_final_github\\Limpiezadatos.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sin_nulos.drop(columns=columnas_a_eliminar, inplace=True)\n",
      "c:\\Users\\jpcan\\OneDrive\\Documentos\\Andes Universidad\\Analitica computacional\\Proyecto_final_github\\Limpiezadatos.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sin_nulos[col] = pd.to_numeric(df_sin_nulos[col], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# Importar las librerías necesarias\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "#import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from Limpiezadatos import df_sin_nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Dividir en X e Y\n",
    "Y = df_sin_nulos['punt_sociales_ciudadanas']\n",
    "columnas_a_excluir = ['punt_ingles','punt_matematicas', 'punt_sociales_ciudadanas', 'punt_c_naturales', 'punt_lectura_critica','cole_mcpio_ubicacion','cole_nombre_sede','periodo']\n",
    "#'punt_matematicas', 'punt_sociales_ciudadanas', 'punt_c_naturales', 'punt_lectura_critica'\n",
    "X = df_sin_nulos.drop(columns=columnas_a_excluir, axis=1)\n",
    "# Convertir las columnas categóricas en variables dummy\n",
    "X_dummies = pd.get_dummies(X)\n",
    "X_dummies = X_dummies.astype(int)\n",
    "\n",
    "\n",
    "# Convertir a arrays de numpy\n",
    "X_dummies = np.array(X_dummies)\n",
    "Y = np.array(Y)\n",
    "\n",
    "\n",
    "\n",
    "X_dummies = X_dummies[:30000]\n",
    "Y = Y[:30000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  0  1 ...  1  0  1]\n",
      " [19  0  1 ...  1  1  0]\n",
      " [16  0  1 ...  0  0  1]]\n",
      "[[-0.18332631 -0.08522476  0.08522476 ...  0.34171142 -0.28815359\n",
      "   0.28815359]\n",
      " [ 0.35874284 -0.08522476  0.08522476 ...  0.34171142  3.47037149\n",
      "  -3.47037149]\n",
      " [-0.45436089 -0.08522476  0.08522476 ... -2.92644595 -0.28815359\n",
      "   0.28815359]]\n"
     ]
    }
   ],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X_dummies, Y, test_size=0.3, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.3, random_state=42)\n",
    "\n",
    "# Escalar los datos (opcional, pero es común en redes neuronales)\n",
    "std_scl = StandardScaler()\n",
    "std_scl.fit(X_train)\n",
    "\n",
    "print(X_train[0:3,])\n",
    "X_train = std_scl.transform(X_train)\n",
    "print(X_train[0:3,])\n",
    "X_valid = std_scl.transform(X_valid)\n",
    "X_test = std_scl.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jpcan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:277: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jpcan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jpcan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jpcan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Mejor configuración:\n",
      "Número de capas ocultas: 3\n",
      "Número de neuronas por capa: 200\n",
      "Loss: 94.68775177001953\n",
      "Mean Absolute Error: 7.654648303985596\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import MeanSquaredError  # Cambio en la función de pérdida\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Definir los datos de entrenamiento y validación (X_train, X_valid, y_train, y_valid)\n",
    "# Supongamos que ya están definidos\n",
    "\n",
    "# Definir la configuración de hiperparámetros a explorar\n",
    "hidden_layers = [1, 2, 3]  # Cantidad de capas ocultas\n",
    "neurons_per_layer = [200, 128, 500]  # Número de neuronas por capa\n",
    "\n",
    "# Inicializar un diccionario para almacenar los resultados\n",
    "results = {}\n",
    "\n",
    "# Iterar sobre las combinaciones de hiperparámetros\n",
    "for num_layers, num_neurons in itertools.product(hidden_layers, neurons_per_layer):\n",
    "    # Crear modelo\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons, input_shape=(X_train.shape[1],), activation=\"relu\"))\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(Dense(num_neurons, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    model.add(Dense(1, activation='softplus'))\n",
    "\n",
    "    # Compilar modelo\n",
    "    model.compile(loss=MeanSquaredError(),  \n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=[\"mean_absolute_error\"]) \n",
    "\n",
    "    # Entrenar modelo\n",
    "    history = model.fit(X_train, y_train, epochs=10,\n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        verbose=0)\n",
    "\n",
    "    # Evaluar modelo en datos de validación\n",
    "    val_loss, val_mae = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "\n",
    "    # Almacenar los resultados en el diccionario\n",
    "    results[(num_layers, num_neurons)] = {'loss': val_loss, 'mean_absolute_error': val_mae, 'model': model}\n",
    "\n",
    "# Encontrar la mejor configuración\n",
    "best_configuration = min(results, key=lambda x: results[x]['loss'])\n",
    "best_model = results[best_configuration]['model']\n",
    "\n",
    "print(\"Mejor configuración:\")\n",
    "print(\"Número de capas ocultas:\", best_configuration[0])\n",
    "print(\"Número de neuronas por capa:\", best_configuration[1])\n",
    "print(\"Loss:\", results[best_configuration]['loss'])\n",
    "print(\"Mean Absolute Error:\", results[best_configuration]['mean_absolute_error'])\n",
    "\n",
    "# Serializar el mejor modelo a un archivo .keras\n",
    "best_model.save('C:/Users/jpcan/OneDrive/Documentos/Andes Universidad/Analitica computacional/Proyecto_final_entregables/Modelos/Mejor_modelo_sociales.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Preparar los datos para el gráfico de barras\n",
    "configurations = list(results.keys())\n",
    "mae_values = [results[config]['mean_absolute_error'] for config in configurations]\n",
    "x_ticks = [f'{config[0]} capas, {config[1]} neuronas' for config in configurations]\n",
    "\n",
    "# Crear el gráfico de barras\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(x_ticks, mae_values, color='white', edgecolor='green')\n",
    "\n",
    "# Resaltar la mejor combinación en verde oscuro\n",
    "best_index = configurations.index(best_configuration)\n",
    "bars[best_index].set_color('darkgreen')\n",
    "\n",
    "# Añadir etiquetas y título\n",
    "plt.xlabel('Configuración')\n",
    "plt.ylabel('Mean Absolute Error (MAE)')\n",
    "plt.title('Comparación de configuraciones según MAE')\n",
    "\n",
    "# Rotar etiquetas del eje x para mejor visualización\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
